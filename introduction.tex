% The aim: general robotics
Robots are becoming ubiquitous in manufacturing and other industries, for a variety of tasks such as bin picking, assembly, welding, painting, and so on.
Yet, the autonomous capability of present-day robotics systems are still quite limited.
Settings where robots operate are carefully controlled; they often require very specific end-effector tooling~\citep{wang2020soft} combined with high precision motions and motion planning~\citep{ang2005pid, lavalle2006planning, karaman2011rrtstar, zucker2013chomp} to accomplish a particular task.
In effect, robots rely on human ingenuity and engineering in order to do their job.
But such systems are brittle, and the hardware and software must often be redesigned for slight variations of a task.
Some adaptability or autonomy can be achieved with compliant robot controllers~\citep{mason1981compliance, hogan1985impedance}, and with abstract task planning~\citep{sacerdoti1974planning, Kaelbling2011} combined with perception.
But if a manufacturing task actually requires significant adaptability or robustness to varying environment conditions based on perceptual inputs, designing a working system becomes much more difficult - as evidenced by the millions of human laborers doing these jobs today.
And beyond relatively controlled manufacturing settings, we will expect the robots of tomorrow to do a lot more: cook meals, assist the elderly in homes and other human-centric environments, navigate unmapped terrain, operate machinery and appliances, manipulate objects, and interact safely in presence of humans. 
This kind of open-world capability requires adaptability, generalization, and is beyond the reach of most robots today.

In contrast, humans perform highly skillful dexterous manipulation so easily that it is sometimes hard to conceive the difficulty of replicating this capability in a robot.
% We can - sometimes literally - do these tasks in our sleep.
Most humans within the first five years of their life have developed complex fine motor skills, successfully performing bimanual dexterous manipulation of various unfamiliar and dynamic objects, and using tools with a tight sensorimotor loop that entails perception, functional grasping, and control~\citep{adolph2017motor}.
It remains a challenge to develop equivalently robust feedback controllers for robots that can adapt to a wide variety of situations to accomplish goals.
If robots were equally skillful, it would be incredibly economically valuable - they could be used to automate many of the tasks that humans have to do today.
How can we develop methods to allow for general-purpose robots that are similarly skillful?

% Existing work in robotics and the shortcomings
% Existing work in robotics focuses on accomplishing well-specified tasks in carefully controlled environments.

% Deep learning
The past decade of deep learning suggests that learning models from large datasets is the key to such open-world generalization, which is a prerequisite for general robotics.
Expressive function approximation trained on broad datasets have driven recent progress in artificial intelligence research across a range of fields: in speech recognition~\citep{Graves2014}, image classification~\citep{krizhevsky2012imagenet} and segmentation~\citep{ren2015fasterrcnn}, natural language processing~\citep{devlin2019bert}, and even protein structure prediction~\citep{jumper2021alphafold}, the recipe of large datasets combined with appropriate deep learning architectures has pushed forward the frontier.
These models are trained on a broad enough dataset that the model can generalize from a broad training distribution and capture corner cases at test time, a challenge with manually designed solutions.
If we could achieve such generality for control - the problem of selecting actions in order to maximize a reward function - it could enable truly general robots in the wild.

% Deep reinforcement learning successes
But control introduces two new problems not found in the supervised learning setting.
The first problem is credit assignment: actions taken in the past affect the future.
The second is exploration: actions taken change the distribution of data visited.
To address these problems, a promising approach is deep reinforcement learning (RL), which combines reinforcement learning with deep function approximation.
Deep reinforcement learning has been applied successfully on many sequential decision making problems: to achieve super-human performance on competitive games such as Atari~\citep{mnih2015human}, Go~\citep{silver2016alphago}, Dota 2~\citep{openai2019dota}, and Starcraft II~\citep{vinyals2019starcraft}, in robotics~\citep{deisenroth2011pilco, kober2013reinforcement, levine2017grasping, lee2020locomotion}, navigation of stratospheric balloons~\cite{bellemare2020loon}, and even control of plasma in a nuclear fusion reactor~\cite{degrave2022fusion}.
Yet, while algorithms for RL have been steadily advancing~\citep{schulman2015trpo, lillicrap2015continuous, schulman2017ppo, haarnoja2018sac}, becoming more sample efficient and stable, there are still significant obstacles towards a general solution for robotics with RL.
What challenges remain toward endowing robots with human-level manipulation skills?

\vspace{5mm}

% What challenges remain: the chicken and the egg problem between data and policy
\textbf{Challenges in robot learning.} The central issue is that the world is so varied that a robotic agent acquiring skills via learning based methods needs to experience both the diversity of perceptual inputs found in large-scale vision datasets, combined with the complexity of control including exploration and credit assignment.
% collect its own data - but thereâ€™s a chicken and the egg problem between the policy and data. 
But unlike the supervised learning setting where collecting and labeling data is relatively straightforward, collecting diverse and useful data in robotics is much harder and more expensive.
Existing work has used demonstrations, which require significant human effort, or run online policy learning in heavily instrumented environments which also requires effort to design reset mechanisms and reward engineering.
Instead, robots would ideally be able to collect their own data continuously with little human supervision in diverse environments as well as utilize past data to learn a policy that generalizes well to diverse environments.
% Autonomously collecting useful data in varied environments requires a policy that generalizes well; training a policy that generalizes well requires a large, diverse dataset.
How can we bootstrap this cycle, in which robots can successfully use prior data to explore and learn in diverse environments?
Broadly, two key components need to be developed for a solution.
% To do so, we face several challenges. When a robot is dropped in a new environment, it must be able to use its prior knowledge to think of potentially useful behaviors that the environment affords. Then, the robot has to be able to actually practice these behaviors informatively. To now improve itself in the new environment, the robot must then be able to evaluate its own success somehow without an externally provided reward.

% Challenge: utilizing prior data
First, we must be able to embed prior knowledge - data of offline experience or demonstrations, human-engineered controllers, and environment models, into the learning process.
The classic active formulation of RL learning from scratch necessitates a lengthy active exploration process for each behavior, making it difficult to apply in real-world settings such as robotic control.
Beyond requiring a large amount of data, the initial exploration phase in RL can also be unsafe to execute on a robot.
Instead of learning tasks from scratch, we ought to be able to utilize existing knowledge to make RL algorithms sample efficient and practical for running in the real world.
If we can instead allow RL algorithms to effectively use prior knowledge to aid the online learning process, such applications could be made substantially more practical, providing a starting point that mitigates challenges due to exploration and sample complexity.
But vitally, online training may still be necessary for the agent to perfect the desired skill, if perfect zero-shot generalization is not achieved.
In this thesis, we explore several ideas in order to incorporate prior knowledge into policy optimization, significantly improving the sample complexity, reducing unsafe exploration behavior, and making these algorithms more amenable for real world use.

% Challenge: goal specification
Second, we must address the problem of task specification.
While reinforcement learning provides an appealing formalism for learning individual skills, a general-purpose robotic system must be able to master an extensive repertoire of behaviors.
Moreover, it must correctly evaluate itself on whether it has succeeded or failed at accomplishing a task, and be able to be tasked with a specific desired goal by a human when needed.
Ideally it could also transfer knowledge between tasks.
% Instead of learning a large collection of skills individually, we instead want to enable a robot to practice many behaviors, and be able to repurpose the learned  knowledge once a new task is commanded by the user?
Goal-conditioned reinforcement learning provides a potential solution to this problem: by conditioning the policy on a continuous goal space, we can enable this transfer and specify tasks in the goal space.
For scalable robot learning, we also want the learning procedure to be \textit{self-supervised}: the robot should be able to evaluate its own success in order to practice, and also autonomously practice useful skills by setting goals that are feasible but diverse.
% When the robot's environment and available objects vary, as they do in most open-world settings, the robot must propose to itself only those goals that it can accomplish in its present setting with the objects that are at hand.
% - where does the reward function and supervision come from? This question leads to the framework of self-supervised RL. Here, we deal with being able to evaluate a reward function for many goals $g$ that the agent may be tasked with, how to infer the distribution of potential goals to train on so you can generalize to test tasks, and how goals interact with policy learning. 

% Vision for robotics if these are solved
If we can overcome these challenges reliably, we can allow robots to use their prior experience to learn general policies, and then when tasked with a new objective, practice and improve through self-supervision.
Moreover, the ability to explore in new environments can be the basis to bootstrap a cycle in which our agents use prior experience to learn a policy to collect high quality interaction data, improve the policy from that data, and so on.
This thesis first explores facets of each challenge individually, demonstrating new algorithms on real-world robotic systems.
We then integrate these solutions into real-world robotic systems that understand affordances and autonomously improve on novel tasks.

\vspace{5mm}

% Outline of thesis
\textbf{Outline.} This thesis is organized into three parts. In Part I (chapter 2-4), we investigate the use of goal-conditioned reinforcement learning for self-supervised exploration from raw observations. In chapter~\ref{chapter:rig}, we describe the framework of reinforcement learning with imagined goals (RIG), which enables self-supervised practice~\citep{nair2018rig}. In chapter~\ref{chapter:skewfit}, we discuss an extension of RIG to improve exploration in self-supervised goal-conditioned RL~\citep{pong2019skewfit}. In chapter~\ref{chapter:ccrig}, we discuss extending RIG to autonomously set goals in novel situations based on prior experience, using a context-conditioned generative model~\citep{nair2019ccrig}.

In Part 2 (chapter 5-10), we discuss utilizing prior data and prior knowledge in order to initialize and accelerate reinforcement learning. In chapter~\ref{chapter:bcddpg}, we discuss utilizing demonstrations to solve long-horizon tasks with reinforcement learning~\citep{nair2018demonstrations}. In chapter~\ref{chapter:awac}, we discuss an algorithm to utilize arbitrary offline data and finetune policies and value functions online~\citep{nair2020awac}. In chapter~\ref{chapter:iql}, we discuss utilizing expectile regression for stable offline learning of value functions. The resulting algorithm, implicit Q-learning, achieves state-of-the-art results in both offline RL and online finetuning~\citep{kostrikov2021iql}. In chapter~\ref{chapter:residualrl}, we discuss how to incorporate prior knowledge such as an expert controller using residual reinforcement learning~\citep{johannink18residualrl}, and in chapter~\ref{chapter:insertion} we further extend residual reinforcement learning to solve industrial insertion tasks~\citep{schoettler2019insertion}. In chapter~\ref{chapter:daib}, we apply implicit Q-learning along with domain generalization to learn reward models and policies for industrial insertion, enabling on-the-job learning when a policy cannot solve an insertion task zero-shot. 

Finally, in Part 3 (chapter 11-12), we show how these two directions dovetail to enable robots in the real world to explore novel situations. In chapter~\ref{chapter:val}, we cover visuomotor affordance learning (VAL), a method to allow self-supervised learning from prior data~\citep{Khazatsky2021WhatCI}. In chapter~\ref{chapter:ptp}, we extend VAL with planning to enable finetuning of more complex skills. In chapter~\ref{chapter:conclusion}, we conclude by discussing future directions.
