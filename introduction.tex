% The aim: general robotics
Robots are becoming ubiquitous in manufacturing and other industries, for a variety of tasks such as bin picking, welding, painting, assembly, and so on.
Yet, the autonomous capability of present-day robotics systems are still quite limited.
Settings where robots operate are carefully controlled; they often require very specific end-effector tooling combined with high precision motions to do a particular task.
In effect, robots rely on human ingenuity and engineering in order to do their job.
But such systems are brittle, and the hardware and software must often be redesigned for slight variations of a task.
If a manufacturing task actually requires adaptability or robustness to varying environment conditions, designing a working system becomes much more difficult.
And beyond manufacturing, we will expect the robots of tomorrow to do a lot more: cook meals, assist the elderly in homes and other human-centric environments, navigate unmapped terrain, operate machinery and appliances, manipulate objects, and interact safely in presence of humans. 
This kind of open-world capability requires adaptability, generalization, and is beyond the reach of most robots today.

In contrast, humans perform highly skillful dexterous manipulation so easily that it is sometimes hard to conceive the difficulty of replicating this capability in a robot.
Most humans within the first five years of their life have developed complex fine motor skills, successfully performing bimanual dexterous manipulation of all kinds of unfamiliar and dynamic objects, and using tools with a tight sensorimotor loop that entails perception, functional grasping, and control~\citep{adolph2017motor}.
It remains a challenge to develop equivalently robust feedback controllers for robots that can adapt to a wide variety of situations to accomplish goals.
If robots were equally skillful, it would be incredibly economically valuable - they could be used to automate many of the tasks that humans have to do today.
How can we develop methods to allow robots to become similarly skillful?

% Existing work in robotics and the shortcomings
% Existing work in robotics focuses on accomplishing well-specified tasks in carefully controlled environments.

% Deep learning
The past decade of deep learning suggests that learning from large datasets is the key to such open-world generalization for robots.
Expressive models that are trained on broad datasets have driven recent progress in artificial intelligence research.
These models are trained on a broad enough dataset to capture corner cases within its training distribution.
Beginning with the breakthrough of AlexNet in image classification, the recipe of large datasets combined with large models has resulted in human-level performance in vision~\citep{krizhevsky2012imagenet}.
Similarly, in NLP, pretraining on large datasets dominates~\citep{devlin2019bert}.
These models have contributed to significant progress in protein folding, 
% However, in robotics, such data sharing and model sharing have proven to be relatively difficult.
If we could achieve such generality for control - the problem of selecting actions over time in order to maximize a reward function - it could enable truly general robots in the wild.

% Deep reinforcement learning successes
But control introduces two new challenges not found in the supervised learning setting.
The first challenge is credit assignment: actions taken in the past affect the future.
The second is exploration: actions taken change the distribution of data visited.
To address these challenges, a promising line of work is deep reinforcement learning.
Deep reinforcement learning has been applied successfully for super-human performance on games such as Atari~\citep{mnih2015human}, Go~\citep{silver2016alphago}, Dota, and Starcraft.
It has been used for robotics, stratospheric balloons, and even control of plasma in a nuclear fusion reactor.
Yet, while algorithms for RL have been steadily advancing, becoming more sample efficient and stable, there are still significant obstacles towards truly solving robotics with RL.
What challenges remain toward endowing robots with human-level skills?

\vspace{5mm}

% What challenges remain: the chicken and the egg problem between data and policy
\textbf{Challenges in robot learning.} The central issue is that the world is so varied that a robotic agent acquiring skills via learning based methods needs to experience both the diversity of perceptual inputs, combined with the complexity of control including exploration and credit assignment.
collect its own data - but thereâ€™s a chicken and the egg problem between the policy and data. You need a policy that generalizes to collect useful data in varied environments, but you need good varied data to obtain such a policy. 
To do so, we face several challenges. When a robot is dropped in a new environment, it must be able to use its prior knowledge to think of potentially useful behaviors that the environment affords. Then, the robot has to be able to actually practice these behaviors informatively. To now improve itself in the new environment, the robot must then be able to evaluate its own success somehow without an externally provided reward.

% Challenge: utilizing prior data
First, speeding up policy optimization from prior knowledge: usually prior data like offline experience or demonstrations, but also things like environment models and human-engineered controllers. The key idea is that instead of learning tasks from scratch, we ought to be able to utilize existing knowledge to make RL algorithms sample efficient and practical for running in the real world.

% Challenge: goal specification
Second, where does the reward/supervision come from? which leads to the framework of self-supervised RL. So here, we deal with being able to evaluate a reward function for many goals g that the agent may be tasked with, how to infer the distribution of potential goals to train on so you can generalize to test tasks, and how this interacts with policy learning.

% Vision for robotics if these are solved
If we can overcome these challenges reliably, we can bootstrap a cycle in which our agents use prior experience to learn a policy to collect high quality interaction data, improve the policy from that data, and so.

\vspace{5mm}

% Outline of thesis
\textbf{Outline.} This thesis is organized as follows. In Part I (Chapter 2-4), we explore utilizing goal-conditioned reinforcement learning for self-supervised exploration from raw observations. In chapter 2, we describe the framework of reinforcement learning with imagined goals (RIG), which enables self-supervised practice. In chapter 3, we discuss an extension of RIG to improve exploration in self-supervised goal-conditioned RL. In chapter 4, we discuss extending RIG to learn in novel situations, using a context-conditioned variation autoencoder (CCVAE). In Part 2 (Chapter 5-8), we discuss utilizing prior data and prior knowledge in order to initialize and accelerate reinforcement learning. In chapter 5, we discuss utilizing demonstrations to solve long-horizon tasks with reinforcement learning. In chapter 6, we discuss how to incorporate prior knowledge such as an expert controller using residual reinforcement learning. In chapter 7, we discuss an algorithm, advantage weighted actor critic (AWAC) to utilize arbitrary offline data, and finetune policies and Q functions online. In chapter 8, we discuss a significant improvement to AWAC, implicit Q-learning (IQL) that is state of the art in both offline RL and online finetuning. Finally, in Part 3 (Chapter 9-10), we show how these two directions come together to enable robots in the real world to explore novel situations. In chapter 9, we cover visuomotor affordance learning (VAL), a method to allow self-supervised learning from prior data. In chapter 10, we extend VAL with planning to enable finetuning of more complex skills. In chapter 11, we discuss some future directions.



% RIG
% For an autonomous agent to fulfill a wide range of user-specified goals at test time, it must be able to learn broadly applicable and general-purpose skill repertoires.
% Furthermore, to provide the requisite level of generality, these skills must handle raw sensory input such as images.
% In this paper, we propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies.
% Since the particular goals that might be required at test-time are not known in advance, the agent performs a self-supervised ``practice'' phase where it imagines goals and attempts to achieve them.
% We learn a visual representation with three distinct purposes: sampling goals for self-supervised practice, providing a structured transformation of raw sensory inputs, and computing a reward signal for goal reaching.
% We also propose a retroactive goal relabeling scheme to further improve the sample-efficiency of our method.
% Our off-policy algorithm is efficient enough to learn policies that operate on raw image observations and goals for a real-world robotic system, and substantially outperforms prior techniques.


% SkewFit
% Autonomous agents that must exhibit flexible and broad capabilities will need to be equipped with large repertoires of skills.
% Defining each skill with a manually-designed reward function limits this repertoire and imposes a manual engineering burden.
% Self-supervised agents that set their own goals can automate this process, but designing appropriate goal setting objectives can be difficult, and often involves heuristic design decisions.
% In this paper, we propose a formal exploration objective for goal-reaching policies that maximizes state coverage.
% We show that this objective is equivalent to maximizing goal reaching performance together with the entropy of the goal distribution, where goals correspond to full state observations.
% To instantiate this principle, we present an algorithm called \METHOD for learning a maximum-entropy goal distributions and prove that, under regularity conditions, \METHOD converges to a uniform distribution over the set of valid states, even when we do not know this set beforehand.
% Our experiments show that combining \METHOD for learning goal distributions with existing goal-reaching methods outperforms a variety of prior methods on open-sourced visual goal-reaching tasks and that \METHOD enables a real-world robot to learn to open a door, entirely from scratch, from pixels, and without any manually-designed reward function.

% CCRIG
% While reinforcement learning provides an appealing formalism for learning individual skills, a general-purpose robotic system must be able to master an extensive repertoire of behaviors. Instead of learning a large collection of skills individually, can we instead enable a robot to propose and practice its own behaviors automatically, learning about the affordances and behaviors that it can perform in its environment, such that it can then repurpose this knowledge once a new task is commanded by the user? In this paper, we study this question in the context of self-supervised goal-conditioned reinforcement learning. A central challenge in this learning regime is the problem of goal setting: in order to practice useful skills, the robot must be able to autonomously set goals that are feasible but diverse. When the robot's environment and available objects vary, as they do in most open-world settings, the robot must propose to itself only those goals that it can accomplish in its present setting with the objects that are at hand. Previous work only studies self-supervised goal-conditioned RL in a single-environment setting, where goal proposals come from the robot's past experience or a generative model are sufficient. In more diverse settings, this frequently leads to impossible goals and, as we show experimentally, prevents effective learning. We propose a conditional goal-setting model that aims to propose goals that are feasible from the robot's current state. We demonstrate that this enables self-supervised goal-conditioned off-policy learning with raw image observations in the real world, enabling a robot to manipulate a variety of objects and generalize to new objects that were not seen during training.



% BCDDPG
% Exploration in environments with sparse rewards has been a persistent problem in reinforcement learning (RL). Many tasks are natural to specify with a sparse reward, and manually shaping a reward function can result in suboptimal performance. However, finding a non-zero reward is exponentially more difficult with increasing task horizon or action dimensionality. This puts many real-world tasks out of practical reach of RL methods. In this work, we use demonstrations to overcome the exploration problem and successfully learn to perform long-horizon, multi-step robotics tasks with continuous control such as stacking blocks with a robot arm. Our method, which builds on top of Deep Deterministic Policy Gradients and Hindsight Experience Replay, provides an order of magnitude of speedup over RL on simulated robotics tasks. It is simple to implement and makes only the additional assumption that we can collect a small set of demonstrations. Furthermore, our method is able to solve tasks not solvable by either RL or behavior cloning alone, and often ends up outperforming the demonstrator policy.

% AWAC
% Reinforcement learning (RL) provides an appealing formalism for learning control policies from experience. However, the classic active formulation of RL necessitates a lengthy active exploration process for each behavior, making it difficult to apply in real-world settings such as robotic control. If we can instead allow RL algorithms to effectively use previously collected data to aid the online learning process, such applications could be made substantially more practical: the prior data would provide a starting point that mitigates challenges due to exploration and sample complexity, while the online training enables the agent to perfect the desired skill. Such prior data could either constitute expert demonstrations or, more generally, sub-optimal prior data that illustrates potentially useful transitions. While a number of prior methods have either used optimal demonstrations to bootstrap reinforcement learning, or have used sub-optimal data to train purely offline, it remains exceptionally difficult to train a policy with potentially sub-optimal offline data and actually continue to improve it further with online RL. In this paper we systematically analyze why this problem is so challenging, and propose an algorithm that combines sample-efficient dynamic programming with maximum likelihood policy updates, providing a simple and effective framework that is able to leverage large amounts of offline data and then quickly perform online fine-tuning of RL policies. We show that our method, advantage weighted actor critic (AWAC), enables rapid learning of skills with a combination of prior demonstration data and online experience. We demonstrate these benefits on a variety of simulated and real-world robotics domains, including dexterous manipulation with a real multi-fingered hand, drawer opening with a robotic arm, and rotating a valve. Our results show that incorporating prior data can reduce the time required to learn a range of robotic skills to practical time-scales.

% Residual RL
% Conventional feedback control methods can solve various types of robot control problems 
% very efficiently by capturing the structure with explicit models, such as rigid body equations of motion.
% However, many control problems in modern manufacturing deal with contacts and friction, which are difficult to capture with first-order physical modeling. 
% Hence, applying control design methodologies to these kinds of problems often results in brittle and inaccurate controllers, which have to be manually tuned for deployment.
% Reinforcement learning (RL) methods have been demonstrated to be capable of learning continuous robot controllers from interactions with the environment, even for problems that include friction and contacts.
% In this paper, we study how we can solve difficult control problems in the real world by decomposing them into a part that is solved efficiently by conventional feedback control methods, and the residual which is solved with RL. 
% The final control policy is a superposition of both control signals.
% We demonstrate our approach by training an agent to successfully perform a real-world block assembly task involving contacts and unstable objects.

% Insertion
% Connector insertion and many other tasks commonly found in modern manufacturing settings involve complex contact dynamics and friction.
% Since it is difficult to capture related physical effects with first-order modeling, traditional control methods often result in brittle and inaccurate controllers, which have to be manually tuned.
% Reinforcement learning (RL) methods have been demonstrated to be capable of learning controllers in such environments from autonomous interaction with the environment, but running RL algorithms in the real world poses sample efficiency and safety challenges.
% Moreover, in practical real-world settings we cannot assume access to perfect state information or dense reward signals.
% In this paper, we consider a variety of difficult industrial insertion tasks with visual inputs and different natural reward specifications, namely sparse rewards and goal images.
% We show that methods that combine RL with prior information, such as classical controllers or demonstrations, can solve these tasks from a reasonable amount of real-world interaction.

% IQL
% Offline reinforcement learning requires reconciling two conflicting aims: learning a policy that improves over the behavior policy that collected the dataset, while at the same time minimizing the deviation from the behavior policy so as to avoid errors due to distributional shift. This trade-off is critical, because most current offline reinforcement learning methods need to query the value of unseen actions during training to improve the policy, and therefore need to either constrain these actions to be in-distribution, or else regularize their values. We propose a new offline RL method that never needs to evaluate actions outside of the dataset, but still enables the learned policy to improve substantially over the best behavior in the data through generalization.
% The main insight in our work is that, instead of evaluating unseen actions from the latest policy, we can approximate the policy improvement step implicitly by treating the state value function as a random variable, with randomness determined by the action (while still integrating over the dynamics to avoid excessive optimism), and then taking a state conditional upper expectile of this random variable to estimate the value of the best actions in that state. This leverages the generalization capacity of the function approximator to estimate the value of the best available action at a given state without ever directly querying a Q-function with this unseen action.
% Our algorithm alternates between fitting this upper expectile value function and backing it up into a Q-function, without any explicit policy. Then, we extract the policy via advantage-weighted behavioral cloning, which also avoids querying out-of-sample actions. We dub our method \ournamepref Q-learning (\ourname).  \ourname is easy to implement, computationally efficient, and only requires fitting an additional critic with an asymmetric L2 loss.
% % \footnote{Our implementation is available at \href{https://github.com/ikostrikov/implicit\_policy\_improvement}{https://github.com/ikostrikov/implicit\_q\_learning}}
% \ourname demonstrates the state-of-the-art performance on D4RL, a standard benchmark for offline reinforcement learning.
% We also demonstrate that \ourname achieves strong performance fine-tuning using online interaction after offline initialization.

% VAL
% A generalist robot equipped with learned skills must be able to perform many tasks in many different environments. However, zero-shot generalization to new settings is not always possible. When the robot encounters a new environment or object, it may need to finetune some of its previously learned skills to accommodate this change. But crucially, previously learned behaviors and models should still be suitable to accelerate this relearning. In this paper, we aim to study how generative models of possible outcomes can allow a robot to learn visual representations of affordances, so that the robot can sample potentially possible outcomes in new situations, and then further train its policy to achieve those outcomes. In effect, prior data is used to learn what kinds of outcomes may be possible, such that when the robot encounters an unfamiliar setting, it can sample potential outcomes from its model, attempt to reach them, and thereby update both its skills and its outcome model. This approach, visuomotor affordance learning (VAL), can be used to train goal-conditioned policies that operate on raw image inputs, and can rapidly learn to manipulate new objects via our proposed affordance-directed exploration scheme. We show that VAL can utilize prior data to solve real-world tasks such drawer opening, grasping, and placing objects in new scenes with only five minutes of online experience in the new scene.

% PTP
% General-purpose robots require diverse repertoires of behaviors to complete challenging tasks in real-world unstructured environments. To address this issue, goal-conditioned reinforcement learning aims to acquire policies that can reach configurable goals for a wide range of tasks on command. However, such goal-conditioned policies are notoriously difficult and time-consuming to train from scratch. In this paper, we propose Planning to Practice (PTP), a method that makes it practical to train goal-conditioned policies for long-horizon tasks that require multiple distinct types of interactions to solve. Our approach is based on two key ideas. First, we decompose the goal-reaching problem hierarchically, with a high-level planner that sets intermediate subgoals using conditional subgoal generators in the latent space for a low-level model-free policy. Second, we propose a hybrid approach which first pre-trains both the conditional subgoal generator and the policy on previously collected data through offline reinforcement learning, and then fine-tunes the policy via online exploration. This fine-tuning process is itself facilitated by the planned subgoals, which breaks down the original target task into short-horizon goal-reaching tasks that are significantly easier to learn. We conduct experiments in both the simulation and real world, in which the policy is pre-trained on demonstrations of short primitive behaviors and fine-tuned for temporally extended tasks that are unseen in the offline data. Our experimental results show that PTP can generate feasible sequences of subgoals that enable the policy to efficiently solve the target tasks.  
% \footnote{Supplementary video: \href{https://sites.google.com/view/planning-to-practice}{sites.google.com/view/planning-to-practice}}
