\section{Conclusion and Discussion}

We presented PTP, a method for real-world learning of temporally extended skills by utilizing planning and fine-tuning to stitch together skills from prior data.
First, planning is used to convert a long-horizon task into achievable subgoals for a lower level goal-conditioned policy trained from prior data.
Then, the goal-conditioned policy is further fine-tuned with active online interaction, mitigating the distribution shift between the offline data and actual states seen during rollouts.
This procedure allows robots to extend their capabilities autonomously, composing previously seen data into more complicated and useful skills.