% \section{Introduction}

% %% What is the problem?

% Solving long horizon tasks has been an important and interesting challenge in robotics.
% Traditional methods that try to manually engineer primitive skills, then plan over them, can be brittle for two reasons.
% First, engineering the primitive skills that are robust can be difficult.
% Second, connecting the skills together can lead to compounding errors or unmet preconditions if each individual skill is not perfectly robust.
% Robust, generalizing machine learning models in domains such as computer vision and NLP has been attributed to training large models on massive datasets\cite{krizhevsky2012imagenet}.

% Learning methods have the potential to solve 
% Given a prior dataset of behaviors, how can an agent learn to compose them with a small amount of online interaction?

% %% Why is it interesting and important?

% If robots were able to successfully perform 
% Learning long-horizon goals and bootstrapping data collection

% %% Why is it hard? (E.g., why do naive approaches fail?)



% %% Why hasn't it been solved before? (Or, what's wrong with previous proposed solutions? How does mine differ?)

% The combination of conditional generative models and planning for composing skills, and offline RL to learn low-level skills

% %% What are the key components of my approach and results? Also include any specific limitations.



\section{Introduction}

% \subsection{Introduction - Reset-Free}

% %% What is the problem?
% % Large datasets for robotics -> Must be collected on-policy -> Supervision cost -> reset-free data collection
% The success of machine learning in domains such as computer vision and NLP has been attributed to training large models on massive datasets~\cite{krizhevsky2012imagenet}.
% %%SL.12.28: Reasonable way to start, but given that we are submitting this to a robotics venue, maybe the above sentence needs to be preceded by one sentence that explains why we are bothering with learning in the first place (i.e., the flow should be robots need learning to handle complexity -> learning needs large datasets -> therefore we need [whatever])
% In robotics, to be robust to a wide variety of potential situations that a robot may encounter, we analogously need large models and large datasets. Although prior work in robotics has attempted to collect and learn from large datasets~\cite{levine2017grasping} [actionable models, conservative data sharing], these datasets are usually collected manually by humans, or with heuristic policies. But for scalable robot learning, collecting data in diverse environments is exactly the same as solving tasks in diverse environments; robots will have to collect data using their prior experience to generalize and attempt coherent and useful behaviors in new environments.

% What has precluded scalable robot data collection thus far? First, human supervision needs to be provided in the form of reward supervision. This can be alleviated with self-supervised goal-conditioned methods that are general enough to represent a wide variety of tasks and do not require external reward supervision~\cite{schaul2015uva, andrychowicz2017her, nair2018rig}. But a second, and less-considered form of scaffolding that humans provide to robot learning is providing resets to the environment. Without resets, RL agents generally fail to learn, often getting trapped in one area of the state space and suffering from heavily correlated data [cite].
% %%SL.12.28: OK, these paragraphs generally have the right idea, though rhetorically it might need to be refined a bit to get this point across more clearly. Basically, right now the connection from "need lots of data" to "need autonomous RL" is not crystal clear. Perhaps we can be more forceful, eg explicitly say that as long as robotic data collection requires meticulous human oversight, robot datasets will always be limited in size, but robots are autonomous, so shouldn't they instead have an advantage in terms of collecting large datasets (autonomously)? The other problem is that the above kind of just motivates reset-free RL, it doesn't really motivate planning. I expect that comes later, but now the motivation is starting to get long-winded...

% %% Why is it interesting and important?
% % Large scale on-policy learning can be used to learn more general skills, and finetune autonomously to a test skill. %% Addressed in par 1

% %% Why is it hard? (E.g., why do naive approaches fail?)
% % Reset-free learning has not been considered much with goal-conditioned RL which is a natural fit.
% To address this issue, prior work has considered a few different strategies.
% One is to learn a backward controller to reset the environment, but this can also be difficult to learn for the same reasons that plague learning a forward controller [han 2015].
% Another is to attempt to remain in states the agent is confident in, but this can be difficult to guarantee and also slow down learning [leave no trace].
% More recently, reset-free learning has been considered in the context of multi-task or goal-conditioned reinforcement learning, where goals or tasks can be set sequentially to reset each other [abhishek and archit's recent papers].
% However, these goals are set heuristically and the policies are learned from scratch, which is not amenable for real-world robot learning.
% %%SL.12.28: This paragraph comes across as quite weak to me. It kind of just lists a bunch of prior works and then, without any justification, labels them as "heuristic" -- that's not going to be very convincing. It is however OK to defer a detailed discussion of prior work to the related work section, where we can address it in more nuance, but we need to make it very clear in the introduction why the problem is *difficult*. Perhaps this can be a good opportunity to simultaneously bring in a bit of motivation for why we want planning?

% %% Why hasn't it been solved before? (Or, what's wrong with previous proposed solutions? How does mine differ?)
% % In goal-conditioned RL, for resets planning is required.
% In this work, we show how the confluence of offline goal-conditioned RL and planning with a context-conditioned affordance model can enable reset-free learning and largely alleviate the need for human supervision in robot learning.
% %%SL.12.28: This comes out of nowhere, change the previous paragraph so that it provides a bit of motivation for goals and planning, as well as (hopefully) some offline stuff, and then this paragraph will make more sense.
% Offline RL enables policy generalization from large prior datasets to new, unseen environments, allowing for coherent behavior in these new environments.
% Context-conditioned affordance models can be utilized to sequentially set goals that are appropriate and informative from new scenarios that the agent experiences, while preventing the agent from correlated experience.
% %%SL.12.28: The goal of this paragraph should be to motivate the technical components of our approach. This would also be a good place to reference a figure that illustrates the main ideas along with a teaser.

% %% What are the key components of my approach and results? Also include any specific limitations.
% % Planning + reset free enables self-supervised learning of tasks.

% The main contribution of this work is to show that planning with a  context-conditioned affordance model enables self-supervised learning of skills with minimal human intervention.
% We show that with careful architecture choices, planning can significantly improve goal-conditioned RL beyond previously proposed approaches.
% %%SL.12.28: This makes it seem like the main contribution is to show that planning helps with goal-conditioned RL, but that's not really the contribution -- plenty of prior papers have shown that.
% We apply these findings in complex simulation environments and a real-world drawer opening and object grasping environment.
% %%SL.12.28: "apply these findings" sounds pretty weak, can we instead more explicitly state what the method accomplishes that is actually impressive?

Existing robotics methods have struggled to be robust to the vast real-world variation that robots may encounter.
%%SL.1.5: As a minor stylistic point, it can be a bit of a downer to start with a very negative statement like this off the bat. It's important to articulate why the problem is hard, but it's often better to start with a sentence about why the problem is important and then why it's hard ("X is great, but X is hard" vs "you all suck at X")
In contrast, handling real-world variation is relatively successful
%%SL.1.5: Grammar nitpick: "handling ... is successful" implies that the fact that it is handling leads to success, not that it is handling successfully (so just rephrase this to be grammatically correct). I.e., if I say "my ability to steal made me successful" it doesn't mean that I'm successful at stealing, but that the fact I can steal made me successful (at something else presumably)
in domains such as computer vision and NLP, which can be attributed to training large machine learning models on massive datasets~\cite{krizhevsky2012imagenet}.
Although prior work in robotics has attempted to collect and learn from large datasets~\cite{levine2017grasping} [actionable models, conservative data sharing]
%%SL.1.5: would be good to cite some things we are not conflicted with too?
using reinforcement learning, these datasets are usually collected manually by humans, or with heuristic policies.
For scalable robot learning, collecting data in diverse environments is exactly the same as solving tasks in diverse environments.
%%SL.1.5: I'm not sure this follows logically -- one can collect data in many ways, not all of which require solving the task. Maybe make this argument more explicit?
Instead of relying heavily on humans, robots will have to collect data using their prior experience to generalize and attempt coherent and useful behaviors in new environments.
%%SL.1.5: Generally I see the point you are trying to make, and it's an important one, but the last three sentences of this paragraph are quite clumsy, and the argument is not made in a logically coherent manner. I would suggest just rewriting the last half of this paragraph to make your argument more persuasively and more logically.

%%SL.1.5: Something that's a little awkward here: what is the problem we are solving? From the previous paragraph and the first sentence of this paragraph, one gets the impression that the problem is "scalable autonomous robot data collection." But if that's the problem we are solving, then what is the evidence we intend to present illustrating that we've done this? Are we going to experimentally validate that our method is more scalable than some other methods for data collection? That it collects more data or better data? Something here feels a bit disconnected...
What has precluded scalable autonomous robot data collection thus far?
First, much of the prior work in RL requires online data collection.
Using offline RL algorithms that can utilize static datasets, as in other deep learning fields, mitigates this issue.
%%SL.1.5: Above sentence is quite awkwardly phrased -- other deep learning fields don't use offline RL algorithms
Second, human supervision needs to be provided in the form of rewards.
This can be alleviated with self-supervised goal-conditioned methods that are general enough to represent a wide variety of tasks and do not require external reward supervision~\cite{schaul2015uva, andrychowicz2017her, nair2018rig}.
%%SL.1.5: So it's not a problem? Then why has it precluded scalable autonomous robot data collection?
Third, an under-considered form of scaffolding that humans provide to robot learning is providing resets to the environment.
%%SL.1.5: I think there a simpler and more concise way to phrase the above sentence
Without resets, RL agents generally fail to learn, often getting trapped in one area of the state space and suffering from heavily correlated data [cite].
To scale up robot learning in the real world, we need to enable the robot to effectively explore and reset the environment with minimum human intervention.
%%SL.12.28: OK, these paragraphs generally have the right idea, though rhetorically it might need to be refined a bit to get this point across more clearly. Basically, right now the connection from "need lots of data" to "need autonomous RL" is not crystal clear. Perhaps we can be more forceful, eg explicitly say that as long as robotic data collection requires meticulous human oversight, robot datasets will always be limited in size, but robots are autonomous, so shouldn't they instead have an advantage in terms of collecting large datasets (autonomously)? The other problem is that the above kind of just motivates reset-free RL, it doesn't really motivate planning. I expect that comes later, but now the motivation is starting to get long-winded...
%%SL.1.5: The planning part is still not getting motivated well, but I do think the above paragraphs improved a bit. I think you have generally the right idea in terms of the argument to make, but the argument needs to be stronger and more logical, and it needs to more directly motivate the particular thing that we are doing in this paper.

% To address this issue, prior work has considered self-supervised goal-conditioned RL which requires relatively little human supervision. Although this framework does not immediately solve the reset problem,
Reset-free learning has been considered more recently in this context of multi-task or goal-conditioned reinforcement learning, where goals or tasks can be set sequentially to reset each other \cite{Gupta2021ResetFreeRL, Lu2021ResetFreeLL} [abhishek and archit's recent papers].
% One is to learn a backward controller to reset the environment, but this can also be difficult to learn for the same reasons that plague learning a forward controller [han 2015].
% Another is to attempt to remain in states the agent is confident in, but this can be difficult to guarantee and also slow down learning [leave no trace].
% More recently, reset-free learning has been considered in the context of multi-task or goal-conditioned reinforcement learning, where goals or tasks can be set sequentially to reset each other [abhishek and archit's recent papers].
However, these goals are set heuristically and the policies are learned from scratch, which is not amenable for real-world robot learning.
To make autonomous practicing skills minimally expensive, the robot would ideally require only a small amount of initial supervision on what skills to practice, and little supervision as the agent trains.
One way to accomplish this is for the human to provide a small amount of initial and goal states, and the robot to attempt to traverse back and forth between these states [han 2015].
% can practice starting with an offline dataset to bootstrap the skills from and \textit{planning to practice} various skills by traversing between these states.
However, as we show in our experiments, the robot may reach arbitrary and unexpected states that existing goal-conditioned offline RL cannot recover from.
In this work, we investigate how planning over subgoals can alleviate the reset problem to enable robots to automatically learn to solve novel tasks
%%SL.12.28: I think the "solve novel tasks" bit can get us in trouble -- is it actually solving *novel* tasks?
with minimum human intervention.
That is, although the robot may not have perfect skills to begin with and may reach arbitrary and unexpected states, it can \textit{plan to practice}, optimizing a sequence of subgoals in order to practice skills while resetting itself.
%%SL.1.5: I think the above paragraph is too much of a "related work" paragraph that tackles a few individual prior works and talks about why they don't solve the problem. But it's not entirely clear what the problem is or what we are proposing to do at this stage. It would be more effective to rewrite this paragraph and instead use it to motivate what we will be doing -- basically, the above paragraph needs to convince the reader that an algorithm based on goal-conditioned RL and planning will address the challenges laid out in the first two paragraphs.

% % Challenges of RL in robotics: Reward shaping and episodic reset.  
% Reinforcement learning holds the promise of enabling robots to automatically acquire useful skills through interactions in a given environment. However, the effectiveness of applying such trial-and-error paradigms to robotics relies on requirements that involve intensive human labor and expertise. First, the learning process requires informative reward signals that are usually carefully devised or annotated by experts. Second, the agent are usually trained in an episodic manner and needs to be reset to an initial state after each episode, which can require nontrivial manual labor in the real world and expensive computation in the simulation. Such reward shaping and episodic resets often need to be instrumented differently in each new task, which becomes a major bottleneck for applying reinforcement learning in challenging robotics domain. To scale up robot learning in the real world, we need to enable the robot to effectively explore and reset the environment with minimum human intervention. 
% %%SL.12.28: I think the first paragraph for this version of the intro can mostly mirror the design of the previous version of the intro -- it's still important to motivate that we need lots of data and autonomy is a bottleneck in that, though I do think it's nice to start bending the narrative toward planning (by motivating the challenges it eventually will address) right here in the first para the way you did.

% Previous works.
% To combat this challenge, a plurality of methods has been developed in the robot learning domain.
% %%SL.12.28: above sentence kind of doesn't say much, delete?
% Various exploration strategies have been designed to encourage the state coverage by an agent~\citep{NIPS2016_6591, pathakICMl17curiosity, burda2018exploration, conti2018improving} in hard-exploration problems~\citep{maillard2014hard}. Naively covering intermediate states can be insufficient for the agent to connect the dots and discover the final solution. In complicated tasks, it could also be difficult to visit diverse states by directly exploring in the given environment. Recent works have also aimed to avoid frequent resets of the environment using predefined task graphs~\citep{Gupta2021ResetFreeRL}, curriculum learning~\citep{Lu2021ResetFreeLL}.
% % , and conservative exploration~\citep{}. 
% These methods either require either expert knowledge or learning reset-free exploration from scratch. 
% % \kuan{Need a better summary and comparison to the prior work.}
% %%SL.12.28: I'm not sure what the citations to the exploration literature have to do with the reset-free papers. In general, the above paragraph is rather hard to follow -- the point of a paragraph like this is to explain why the problem is hard and why prior work has not solved it, but this is not apparent to me from reading the above.

% Problem Statement.

% In this work, we investigate how planning over subgoals can alleviate the reset problem to enable robots to automatically learn to solve novel tasks
% %%SL.12.28: I think the "solve novel tasks" bit can get us in trouble -- is it actually solving *novel* tasks?
% with minimum human intervention.
% That is, although the robot may not have perfect skills to begin with and may reach arbitrary and unexpected states, it can optimize a sequence of subgoals in order to practice skills while resetting itself.
% Instead of training the policy from scratch, we expect the robot to make use of prior data that contains expert demonstrations of related skills in the same environment.
% When training the robot to solve a novel task, we would like the robot to extract transferable knowledge from such prior data to reduce the required human interventions for reward shaping and episodic resets
% In particular, we consider a goal-conditioned reinforcement learning problem setup where each task is specified by a set of initial states and a set of goal states in an environment. 
% %%SL.12.28: Generally, this is a good paragraph, and it's important to basically have a paragraph like this, but the problem is that it doesn't connect very well to the method -- the goal-conditioned bit kind of comes out of nowhere at the end, and there is no motivation for planning at all. Perhaps somewhere before this paragraph (or at the beginning of this paragraph?) we can add some more discussion that motivates *why* we goal conditioned RL and planning can help with autonomy, and then we can summarize what we do (perhaps with the help of a Figure 1 diagram).
%%AVN I split this paragraph into the top and bottom paragraphs, but if we liked having this ordering of the information we can restore it


% Technical Solution.
To this end, we propose MODEL\_NAME, an approach that autonomously learns to solve novel tasks via learning a goal-conditioned policy from offline data and then optimizing over a sequence of subgoals to collect exploration data autonomously.
%%SL.12.28: The term "planning" won't mean the same thing to your readers taht it means to you, perhaps better explain it as something like learning a goal-conditioned policy and then optimizing over a sequence of goals to issue to this policy
We consider a goal-conditioned reinforcement learning problem setup where each task is specified by a set of initial states and a set of goal states in an environment. 
When training the robot to solve a novel task, we would like the robot to extract transferable knowledge from such prior data to reduce the required human interventions for reward shaping and episodic resets.
In our approach we extract two types of models from the prior data.
First, a goal-conditioned policy is learned offline 
% pretrained by a cutting-edge
%%SL.12.28: doesn't seem relevant that it's cutting edge, since that's not our cnotribution?
% offline reinforcement learning algorithm~\cite{kostrikov2021offline} 
and then finetuned to solve a novel task through online exploration.
Second, we train a generative model that learns to capture the distribution of meaningful subgoals
%%SL.12.28: if it's key to our approach, we should have motivated it in one of the preceding paragraphs
conditioned on the current state from the prior data.
When the robot explores in the novel task, a planner uses the generative model to propose subgoals and sequence them into an informative plan to guide the robot to traverse between the initial states and the goal states.
To reduce the need of episodic resets, our approach iterates between planning towards the initial states and the goal states periodically.
%%SL.1.5: It makes sense to have a paragraph like this that essentially explains our solution, but this should follow a paragraph that motivates the parts (basically, when the reader reads this paragraph, it should already be clear to them why this is a good idea). Additionally, it would be really nice if these design decisions clearly connect to the challenges we articulated in the first two paragraphs -- right now, the bit about resets is kind of an afterthought, and it's not clear how any of this will actually address the problem that the first two paragraphs lay out. This paragraph would also be a good place to reference a figure.
% A goal relabeling mechanism~\citep{andrychowicz2017hindsight} is designed to scaffold the training of the goal-conditioned policy using the collected trajectories guided by the plan. 

%%SL.12.28: State key contributions and then key experimental takeaways.
% Experiments.
The main contribution of this work is to show that planning with a  context-conditioned affordance model
%%SL.1.5: I don't think the notion of an affordance model came up in the previous paragraphs
enables self-supervised learning of skills with minimal human intervention.
%%SL.1.5: I think this is a *very* strong claim that our experiments will fail to back up. Generally, when people see "self-supervised" they mean the robot decides on the tasks -- but in our method we still specify which goals to accomplish and provide prior data that is essentially demonstration data. I think it's a long shot to claim that this is really "self-supervised".
We demonstrate that our approach can efficiently learn to solve unseen
%%SL.1.5: what does it mean to "learn to solve unseen ... tasks"? it kind of sounds like the robot has to perform the task without seeing what it is or something, a bit of a non sequitur
robotic manipulation tasks in simulation and the real world without meticulous reward shaping and with a limited number of episodic resets.
Compared to existing baselines, our approach achieves higher task success rates after the same amount of exploration steps and resets.
%%SL.1.5: The problem is that the above discussion of results is very unclear about the problem setting and the assumptions. This is a problem, because then readers will expect things that you can't deliver. Need to be clearer about precisely what is being solved and how. For example, it's not even clear that prior data is being used, etc.

%%SL.12.28: Generally, I do think the more planning-centric motivation is a bit better, but this version of the intro needs a lot of work to more thoroughly motivate the various design choices.
