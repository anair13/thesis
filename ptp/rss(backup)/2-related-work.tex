\section{Related Work}
\kuan{This section is outdated. We will finish it soon.}

%%SL.2.24: I take a closer look through this section once it's revised, but a very important thing to keep in mind: If you are submitting the paper to a robotics conference, the related work section needs to touch on the most relevant *robotics* work, not just a bunch of deep RL papers. This includes a brief summary of robotic RL papers (could just be a sentence), and possibly also a short discussion of multi-task learning in RL. Try not to just cite things from the past three years, the robotics literature on multi-task learning goes quite far back. It might also help to have a sentence or two about planning in robotics (not planning in the deep RL literature, but actual robotics literature, like Likhachevsky, Kaelbling, Lozano-Perez, etc.) and explain how that's different from what is going on in our work.

We propose to use goal-conditioned reinforcement learning from prior data in order to allow robots to learn in non-episodic settings.
%%SL.1.5: The above sentence is a clearer and more coherent statement of the paper's goals than anything in the intro, which is good on the one hand, but on the other hand, it's a little underwhelming, because if that is really all we are doing, then the work is not novel, because other prior papers have already proposed to do this.
In this section, we cover prior methods in goal-conditioned RL and offline RL, then discuss prior methods in reset-free RL and how they relate to our method.

\textbf{Goal-Conditioned Reinforcement Learning.} The aim of Goal-Conditioned Reinforcement Learning (GCRL) is to control the agent to efficiently reach the specified goal state~\cite{Kaelbling1993LearningTA, Schaul2015UniversalVF, Eysenbach2021CLearningLT}. Compared to policies that are trained to solve a fixed task, the same goal-conditioned policy can perform a variety of tasks when it is commanded with different goals. Such flexibility allows GCRL to better share knowledge across different tasks and make use of goal relabeling techniques to improve the sample efficiency without meticulous reward engineering~\cite{Andrychowicz2017HindsightER, Pong2020SkewFitSS, Fang2019CurriculumguidedHE, Ding2019GoalconditionedIL, Gupta2019RelayPL, Sun2019PolicyCW, Eysenbach2020RewritingHW, Ghosh2021LearningTR}. To facilitate learning to reach distant goals over a long time horizon, recent works propose to plan for a sequence of subgoals to guide the exploration~\cite{Nasiriany2019PlanningWG, Eysenbach2019SearchOT, Charlesworth2020PlanGANMP, Pertsch2020LongHorizonVP, Sharma2021AutonomousRL, Zhang2021CPlanningAA}. The main challenge for such planning methods is to effectively propose and rank achievable subgoals that lead to the final goal. Prior works either sample from existing states from the agent's past experience
%%SL.1.5: maybe put the citations here so it's clear which ones do this (but you can remove the citations before, and instead split them into two groups in this sentence depending on which approach they take)
or generate unseen states from a latent space,
%%SL.1.5: cites
using hand-designed metrics or learning value functions from scratch to rank the feasibility of the plans. In contrast to prior work, our approach learns an affordance model~\cite{Nair2018VisualRL, Nair2019ContextualIG, Khazatsky2021WhatCI, ChaneSane2021GoalConditionedRL}
%%SL.1.5: well, if prior methods also learn an affordance model (as is implied by this citation block), then it's not really "in contrast to prior work", right?
to generate imagined goals conditioned on the observed state, by capturing the distribution of transitions in the prior data. Using the affordance model to recursively generate subgoals, our approach is able to effectively propose feasible plans in novel scenarios. 
%%SL.1.5: I think the above paragraph is generally written quite well, but at the end it doesn't actually leave the reader with a clear idea of how our approach to planning goals differs from prior work (or, put another way, why couldn't we have just directly adapted one of the prior methods, and wouldn't it have solved our problem?)

\textbf{Learning from Prior Data.} Recent advances in offline reinforcement learning have made robotic learning from prior data using RL practical and convenient. Offline RL methods usually add conservatism to the RL objective to make training without collecting additional data more stable~\cite{lange2012batch, fujimoto2019off, kumar2019stabilizing, zhang2021brac, kumar2020conservative, fujimoto2021minimalist}. These methods have been applied in robotic domains to learn skills, including goal-conditioned skills from off-policy data~\cite{kalashnikov2021mtopt, yu2021conservative, chebotar2021actionable}. However, less work has addressed the problem of utilizing prior data and then collecting additional data online for improvement~\cite{nair2020awac, villaflor2020finetuning, lu2021awopt, Khazatsky2021WhatCI, lee2021finetuning, meng2021starcraft}. In this work we focus on the how to do this without human-provided resets. We show that such a reset-free finetuning process with offline initialization can still be effective with planning and the appropriate design decisions.
%%SL.1.5: OK, this paragraph is also very important, but it also suffers from the same problem as the previous one -- there are great citations, but the reader is left unsure how what we are doing differs from these other papers that do offline pretraining + online finetuning. An additional issue is that the intro doesn't really make it crystal-clear that we are using prior data (nor does it really motivate it) -- that's not a problem with this paragraph, but rather a problem with the intro, but it really needs to be fixed.

% \textbf{Non-Episodic Reinforcement Learning.} In the canonical reinforcement learning formulation, the episodic reset enables the agent to breaks the collected experiences into episodes and periodically start over from a state sampled from the initial state probability. Without the episodic reset, the agent would need to autonomously return to a legitimate initial state by itself and avoid getting stuck in the sink states that are hard to recover from \cite{Lu2019AdaptiveOP, CoReyes2020EcologicalRL, Lu2021ResetFreeLL, Sharma2021AutonomousRL}.
% Gupta et al. propose to use a predefined task graph to manual rests performed by humans~\cite{Gupta2021ResetFreeRL}. In the task graph, the terminal state of one task can serve as the initial state of another task. Instead of explicitly utilizing any prior knowledge of the tasks, our approach avoids episodic resets by learning to plan trajectories between the initial states and the goal states.
% %%SL.1.5: it's not clear why planning trajectories avoids the need for resets (also, our model doesn't really "learn to plan" -- it learns, and then plans)
% Similarly, \cite{Nasiriany2019PlanningWG} and \cite{Sharma2021AutonomousRL}
% %%SL.1.5: here and elsewhere, use \citet instead of \cite if you want the parenthetical citation to serve as a noun
% also conduct non-episodic reinforcement learning through planning. Given the learned skill policy and dynamics model, \cite{Nasiriany2019PlanningWG} searches for trajectories that will lead to high cumulative rewards. \cite{Nasiriany2019PlanningWG} focuses on tasks that do not have an explicit goal state and the agent can keep exploring the environment as long as it does not get stuck in the sink states.
% %%SL.1.5: kind of unclear how that's different from what we're doing
% In the goal-reaching tasks, however, the agent would need to turn around after reaching the goal. \cite{Sharma2021AutonomousRL} assumes that the agent starts from the goal state and asks the agent to traverses between the final goal and a subgoal state sampled from its past experiences. It reduces the necessity of episodic resets by progressively choosing the subgoals that are closer the initial state in a curriculum learning manner. In contrast, our approach does not make such assumptions and can generate feasible subgoals that are unseen by the agent. 
% %%SL.1.5: I think we can do better in explaining how our method is different (but let's discuss more on Slack)

\textbf{Planning with Subgoals.} 

In addition to the baselines discussed in Sec.~\ref{sec:experiments}, Visual MPC~\cite{Finn2017DeepVF}, CVAE-SBMP~\cite{ichter2018learning}, and SeCTAR~\cite{co2018self}, several other methods can be used for dynamics model learning. 
% Dynamics models represented as conditional probability distributions of states given actions can be learned through latent models.variable  for different purposes. 
\cite{ichter2018learning} uses conditional variational autoencoder (CVAE) \cite{kingma2013auto, sohn2015learning} to learn a generative model to draw collision-free samples from the action space. This idea is further extended for collision-free motion planning \cite{ichter2019robot}. Co-Reyes et al.~\cite{co2018self} use a policy decoder to generate sequence of actions and a state decoder to predict the dynamics given the same latent feature as the input. The two decoders are jointly trained to encourage the predicted future states are consistent with the resultant states caused by the generated actions. Both decoders receive the same latent feature as input. The main notable difference is that most of these methods represent the probability distribution of a single action mode, where the data is often deliberate for the task. 
% While, the hierarchical dynamics model in the CAVIN Planner decouples the model learning into latent code for effects and motion codes, each of which can guide the action sampling. And finally the consistency action sampling is ensured through dynamics prediction over a self-supervised dataset. 

% \kuan{Remove the non-episodic RL part and add using planning/compositionality for RL.}

% \kuan{Maybe we should merge the three sub-sections and make it more concise, since we are gonna talk about some of these topics in more details in the Background section.}
% \kuan{Maybe we should also discuss prior work on model-free RL augmented with model-based approaches.}