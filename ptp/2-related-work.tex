\section{Related Work}
% \kuan{This section is outdated. We will finish it soon.}

%%SL.2.24: I take a closer look through this section once it's revised, but a very important thing to keep in mind: If you are submitting the paper to a robotics conference, the related work section needs to touch on the most relevant *robotics* work, not just a bunch of deep RL papers. This includes a brief summary of robotic RL papers (could just be a sentence), and possibly also a short discussion of multi-task learning in RL. Try not to just cite things from the past three years, the robotics literature on multi-task learning goes quite far back. It might also help to have a sentence or two about planning in robotics (not planning in the deep RL literature, but actual robotics literature, like Likhachevsky, Kaelbling, Lozano-Perez, etc.) and explain how that's different from what is going on in our work.
%%SL.2.27: For a 6-page paper, we really can't have related work go over the second page. Ideally, Sec I+II would be 1.75 pages max, else we just don't have enough space to explain the method. Try to reduce text and keep citations. More generally, once the particulars are cleaned up, we really need to think about related work strategy. There are a few papers that are extremely relevant, and we need to make it 110% clear why our work is novel relative to these papers (that is really the main thing that matters, the rest is just making sure to cite enough prior work that your reviewers don't get offended about the paper missing some big chunk of prior work)
%%AVN.2.28 looks like we have 8 pages so we don't have to worry so much about space

We propose to use a combination of optimization-based planning and fine-tuning with goal-conditioned reinforcement learning from prior data in order to allow robots to learn temporally extended skills.
In this section, we cover prior methods in offline RL, planning, goal-conditioned RL,
% , offline RL, planning, 
and how they relate to our method.
%%SL.2.27: Maybe delete the above (it just follows from intro), remove paragraph headings, and contract the paragraphs below.

\textbf{Learning from prior data.}
Offline reinforcement learning methods learn from prior data~\cite{lange2012batch, fujimoto2019off, kumar2019stabilizing, zhang2021brac, kumar2020conservative, fujimoto2021minimalist,singh2020cog}, and can also finetune through online interaction~\cite{nair2020awac, villaflor2020finetuning, lu2021awopt, Khazatsky2021WhatCI, lee2021finetuning, meng2021starcraft}. Such methods have been used in a variety of robotic settings~\cite{kalashnikov2018scalable,cabi2019scaling,kalashnikov2021mtopt,lu2021awopt}. Our focus is not on introducing new offline RL methods. Rather, our work shows that planning over subgoals for a goal-conditioned policy that is pretrained offline can enable finetuning for temporally extended skills that would otherwise be very difficult to learn.
%Recent advances in offline reinforcement learning have made robotic learning from prior data using RL practical.
%%SL.3.1: It would be good to add citations to actual robotics papers -- right now it seems like very few of the papers cited in this paragraph actually pertain to robotics.
%Offline RL methods usually add conservatism to the RL objective to make training without collecting additional data more stable~\cite{lange2012batch, fujimoto2019off, kumar2019stabilizing, zhang2021brac, kumar2020conservative, fujimoto2021minimalist}. These methods have been applied in robotic domains to learn skills, including goal-conditioned skills from off-policy data~\cite{kalashnikov2021mtopt, yu2021conservative, chebotar2021actionable}. However, less work has addressed the problem of effectively collecting additional data online for improvement after pre-training on the prior data~\cite{nair2020awac, villaflor2020finetuning, lu2021awopt, Khazatsky2021WhatCI, lee2021finetuning, meng2021starcraft}.
%%SL.3.1: I don't think it's relevant that there is "less" work. It's more important to explain how what we are doing is distinct. In general, I would recommend mostly rewriting this paragraph to focus more on how this all compares to what we are doing rather than having a bunch of red herrings about conservatism. For example, could structure it something like this: Offline reinforcement learning methods learn from prior data~\citep{}, and can also finetune through online interaction~\citep{}. Such methods have been used in a variety of robotic settings, including manipulation~\citep{} and navigation~\citep{}. Our focus is not on introducing new offline RL methods. Rather, our work shows that planning over subgoals for a goal-conditioned policy that is pretrained offline can enable finetuning for temporally extended skills that would otherwise be very difficult to learn.
%In this work we focus on how to fine-tune goal-conditioned policies for long-horizon tasks with minimal human supervision. 
% In the long-horizon case, this requires the use of ideas from sampling-based planning, which we will discuss next.
%%SL.2.27: I don't think we need so much text about offline RL, because it's not really the focus of this work. I also don't think this should come first. I think a better way to do it would be to have a short paragraph at the end of the related work section that basically says "we use offline RL [lots of citations] and we fine-tune [lots of citations] but we don't do anything novel here and instead just use it as a tool" (or something like that, though of course less colloquially)
%%AVN.2.28 The way the intro is written now, I think it actually makes more sense for this to come first - let me know what you think

\textbf{Goal-conditioned reinforcement learning.} The aim of goal-conditioned reinforcement learning (GCRL) is to control the agent to efficiently reach specified goal states~\cite{Kaelbling1993LearningTA, Schaul2015UniversalVF, Eysenbach2021CLearningLT}. Compared to policies that are trained to solve a fixed task, the same goal-conditioned policy can perform a variety of tasks when it is commanded with different goals. Such flexibility allows GCRL to better share knowledge across different tasks and make use of goal relabeling techniques to improve the sample efficiency without meticulous reward engineering~\cite{Andrychowicz2017HindsightER, Pong2020SkewFitSS, Fang2019CurriculumguidedHE, Ding2019GoalconditionedIL, Gupta2019RelayPL, Sun2019PolicyCW, Eysenbach2020RewritingHW, Ghosh2021LearningTR}. Prior has explored various strategies for proposing goals for exploration~\cite{nair2018rig, Nair2019ContextualIG, Khazatsky2021WhatCI, ChaneSane2021GoalConditionedRL}, and studied goal-conditioned RL from offline data~\cite{chebotar2021actionable}. However, such works generally aim to learn short-horizon behaviors, and learning to reach goals that require multiple stages (e.g., several manipulation primitives) is very difficult, as shown in our experiments. Our work aims to extend model-free goal-conditioned RL methods by incorporating elements of planning to enable effective finetuning for multi-stage tasks.

\textbf{Planning.} A wide range of methods have been developed for planning in robotics. At the most abstract level, symbolic task planning searches over discrete logical formulas to accomplish abstract goals~\cite{fikes1971strips}. Motion planning methods solve the geometric problem of reaching a goal configuration with dynamics and collision constraints~\cite{Kavraki1996, koenig2002dstarlite,  karaman2011rrtstar, zucker2013chomp, kalakrishnan2011stomp}. Prior methods have also considered task and motion planning as a combined problem~\cite{srivastava14tamp}. These methods generally assume high-level structured representations of environments and tasks, which can be difficult to actualize in real-world environments. Since in our setting we only have image inputs and not structured scene representations, we focus on methods that can handle raw images for observations and task specification.
%%SL.3.1: The above paragraph is great! I really like how it reads, very nice coverage of prior work and really crisp articulation of how it differs from ours. But still need to address the comment below.
%%SL.2.27: feels like there must be other planning-style methods that we're missing, eg the neurosymbolic programming stuff, etc. -- don't need lots of discussion about it, just a sentence with citations, but it's good to note all the stuff that does something long-horizon

%%SL.3.1: Rewrote the discussion of prior subgoal optimization methods, please reread and check this
\textbf{Combining goal-conditioned RL and planning.}
A number of recent works have sought to integrate concepts from planning with goal-conditioned policies in order to plan sequences of subgoals for longer-horizon tasks~\cite{Nasiriany2019PlanningWG, Eysenbach2019SearchOT, fang2019cavin, Charlesworth2020PlanGANMP, Pertsch2020LongHorizonVP, Sharma2021AutonomousRL, Zhang2021CPlanningAA}. These prior methods either propose subgoals from the set of previously seen states, or directly optimize over subgoals, often by utilizing a latent variable model to obtain a concise representation of image-based states~\cite{nair2018rig,ichter2018learning,nair2019hierarchical,Nasiriany2019PlanningWG,Pertsch2020LongHorizonVP, Khazatsky2021WhatCI, ChaneSane2021GoalConditionedRL}. 
The method we employ is most closely related conceptually to the method proposed by Pertsch et al.~\cite{Pertsch2020LongHorizonVP}, which also employs a hierarchical subgoal optimization, and the method proposed by Nasiriany et al.~\cite{Nasiriany2019PlanningWG}, which also optimizes over sequences of latent vectors from a generative model. Our approach makes a number of low-level improvements, including the use of a conditional generative model~\cite{Nair2019ContextualIG}, which we show leads to significantly better performance. More importantly, our method differs conceptually from these prior works in that our focus is specifically on utilizing subgoal optimization as a way to enable finetuning goal-conditioned policies for longer-horizon tasks. We show that it is in fact this capacity to enable effective finetuning that enables our method to solve more complex multi-stage tasks in our experiments.

%%SL.3.1: All the stuff below is old material for addressing planning, which I rewrote in the paragraph above.
%The main challenge for such planning methods is to effectively propose and rank achievable subgoals that lead to the final goal. Prior works either sample from the set existing states from the agent's past experience or generate unseen states from a latent space,
%using hand-designed metrics or learning value functions from scratch to rank the feasibility of the plans. In contrast to prior work, our approach learns a generative model~\cite{Nair2018VisualRL, nair2019hierarchical, Nair2019ContextualIG, Khazatsky2021WhatCI, ChaneSane2021GoalConditionedRL}
%to propose imagined goals conditioned on the observed state, by capturing the distribution of transitions in the prior data. Using the generative model to recursively propose subgoals, our approach is able to effectively propose feasible plans in novel scenarios.

%These methods for planning over images broadly involve learning latent representations to plan in a more manageable space~\cite{finn2017deepvf, coreyes2018sectar}.
%Ichter et al. use a conditional variational autoencoder (CVAE)~\cite{kingma2014vae, sohn2015cvae} to learn a generative model to draw collision-free samples from the action space~\cite{ichter2018learning}. This idea was further extended for collision-free motion planning \cite{ichter2019robot}. The main notable difference is that most of these methods represent the probability distribution of a single action mode, where the data is often deliberate for the task. 

%Closest to our method, methods have been developed that learn goal-conditioned subgoals to accomplish long-horizon tasks from image input.
%We empirically evaluate these methods in our experiments.
%Pertsch et al. learn a goal-conditioned predictor (GCP) which recursively predicts intermediate subgoals between the initial state and goal state~\cite{Pertsch2020LongHorizonVP}.
%Our method, which uses a generative model to generate plans, can \textit{compose} skills, which GCP cannot due to learning a model that is conditioned on the final state.
%Additionally, GCP uses an inverse model instead of a goal-conditioned policy as the lower-level controller, and empirically performs worse than PTP.
%Nasiriany et al. introduce latent embeddings for abstracted planning (LEAP) which uses optimization-based planning with a variational auto-encoder to handle planning over raw images~\cite{Nasiriany2019PlanningWG}.
%Our method uses the same idea, but using a conditional goal-generation model for planning allows us to plan in visually complex real-world environments, and we demonstrate empirically that our method can handle more complex scenes than LEAP.

% While, the hierarchical dynamics model in the CAVIN Planner decouples the model learning into latent code for effects and motion codes, each of which can guide the action sampling. And finally the consistency action sampling is ensured through dynamics prediction over a self-supervised dataset. 

% \kuan{Remove the non-episodic RL part and add using planning/compositionality for RL.}

% \kuan{Maybe we should merge the three sub-sections and make it more concise, since we are gonna talk about some of these topics in more details in the Background section.}
% \kuan{Maybe we should also discuss prior work on model-free RL augmented with model-based approaches.}

% without human-provided resets. We show that such a reset-free finetuning process with offline initialization can still be effective with planning and the appropriate design decisions.
%%SL.1.5: OK, this paragraph is also very important, but it also suffers from the same problem as the previous one -- there are great citations, but the reader is left unsure how what we are doing differs from these other papers that do offline pretraining + online finetuning. An additional issue is that the intro doesn't really make it crystal-clear that we are using prior data (nor does it really motivate it) -- that's not a problem with this paragraph, but rather a problem with the intro, but it really needs to be fixed.

% \textbf{Non-Episodic Reinforcement Learning.} In the canonical reinforcement learning formulation, the episodic reset enables the agent to breaks the collected experiences into episodes and periodically start over from a state sampled from the initial state probability. Without the episodic reset, the agent would need to autonomously return to a legitimate initial state by itself and avoid getting stuck in the sink states that are hard to recover from \cite{Lu2019AdaptiveOP, CoReyes2020EcologicalRL, Lu2021ResetFreeLL, Sharma2021AutonomousRL}.
% Gupta et al. propose to use a predefined task graph to manual rests performed by humans~\cite{Gupta2021ResetFreeRL}. In the task graph, the terminal state of one task can serve as the initial state of another task. Instead of explicitly utilizing any prior knowledge of the tasks, our approach avoids episodic resets by learning to plan trajectories between the initial states and the goal states.
% %%SL.1.5: it's not clear why planning trajectories avoids the need for resets (also, our model doesn't really "learn to plan" -- it learns, and then plans)
% Similarly, \cite{Nasiriany2019PlanningWG} and \cite{Sharma2021AutonomousRL}
% %%SL.1.5: here and elsewhere, use \citet instead of \cite if you want the parenthetical citation to serve as a noun
% also conduct non-episodic reinforcement learning through planning. Given the learned skill policy and dynamics model, \cite{Nasiriany2019PlanningWG} searches for trajectories that will lead to high cumulative rewards. \cite{Nasiriany2019PlanningWG} focuses on tasks that do not have an explicit goal state and the agent can keep exploring the environment as long as it does not get stuck in the sink states.
% %%SL.1.5: kind of unclear how that's different from what we're doing
% In the goal-reaching tasks, however, the agent would need to turn around after reaching the goal. \cite{Sharma2021AutonomousRL} assumes that the agent starts from the goal state and asks the agent to traverses between the final goal and a subgoal state sampled from its past experiences. It reduces the necessity of episodic resets by progressively choosing the subgoals that are closer the initial state in a curriculum learning manner. In contrast, our approach does not make such assumptions and can generate feasible subgoals that are unseen by the agent. 
% %%SL.1.5: I think we can do better in explaining how our method is different (but let's discuss more on Slack)

