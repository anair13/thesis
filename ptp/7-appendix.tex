%% (Patrick) Moved to experiments section
% \subsection{Implementation Details}
% \label{appendix:implementation_details}
% In PTP, we collect an offline dataset $\mathcal{D}$, train representation learning, train offline RL, and finally run online RL for a specific environment. 

% In the representation learning phase, we first train the VQ-VAE on $\mathcal{D}$ ~\cite{Oord2017NeuralDR}. The VQ-VAE encodes our images from our $[0,1]^{64\times 64\times 3}$ observation space into a length-720 latent vector $\mathbb{z}$. We then encode the entire dataset with the VQ-VAE to obtain discrete latent variables, and then train our conditional subgoal generator on the discrete latent code dataset.

% In the offline RL phase, we run IQL ~\cite{kostrikov2021iql} on the discrete latent code dataset to obtain a single policy and Q-function. This policy and Q-function can then be fine-tuned to a specific environment by running online RL. During training, we relabel the goal with future hindsight experience replay with $60\%$ probability and the next observation with $10\%$ probability.

% All hyper-parameters are provides below for these algorithms in tables ~\cref{tab:rl_hyperparams}, _, _, _, and _.

% \begin{table*}[]
%     \centering
%     \begin{tabular}{c|c}
%       Hyper-parameter  & Value \\
%       \hline
%       $\gamma$  &  $.99$ \\
%       Batch Size & 1024 \\
%       Policy Learning Rate & $3 \dot 10^{-4}$ \\
%       Policy Weight Decay & $0$ \\
%       Q-Network Learning Rate & $3 \dot 10^{-4}$ \\
%       Q-Network Weight Decay & $0$ \\
%       $\beta$ & 0.01 \\
%       $\epsilon$ & 3.0 \\
%     \end{tabular}
%     \caption{Hyper-parameters}
%     \label{tab:hyperparams}
% \end{table*}


% We use a VQ-VAE to encode our images from 3x48x48 to length 720. We modify a PyTorch implementation of  for our offline RL algorithm. Our policy is fully connected network consisting of 4 intermediate layers of size 256. Our Q-network and value network are fully connected networks consisting of 2 intermediate layers of size 256. 


% The hyperparameters used for the experiment are in ~\cref{tab:hyperparams}. We use two replay buffers, one storing online trajectories and one storing offline trajectories, and sample proportionally from the two at training time. 

% \begin{table*}[]
%     \centering
%     \begin{tabular}{c|c}
%       Hyperparameter  & Value \\
%       \hline
%       $\gamma$  &  $.99$ \\
%       Batch Size & 1024 \\
%       Policy Learning Rate & $3 \dot 10^{-4}$ \\
%       Policy Weight Decay & $0$ \\
%       Q-Network Learning Rate & $3 \dot 10^{-4}$ \\
%       Q-Network Weight Decay & $0$ \\
%       $\beta$ & 0.01 \\
%       $\epsilon$ & 3.0 \\
%     \end{tabular}
%     \caption{Hyperparameters}
%     \label{tab:hyperparams}
% \end{table*}




\subsection{Simulation Experimental Details}
\label{appendix:simulation_experimental_details}
Our simulated dataset consists of 4,000 trajectories (300,000 transitions). Every 4 trajectories, we randomize the position and orientation of the drawer, slidable object, and graspable object in the following ways. The position of the drawer is randomly selected between the back two quadrants. The orientation of the drawer is randomly selected such that it doesn't collide into the surrounding wall. The position of the slidable object is randomly selected between the four quadrants such that it doesn't collide with the drawer. The position of the graspable object is randomly selected between inside the drawer, on top of the drawer, or on top of the table. If the graspable object is on top of the table, its position on top of the table is randomly selected such that it doesn't collide with the drawer or slidable object. The yaw of the graspable object is randomized. 

% The trajectories are generated by a scripted policy with added Gaussian noise of .05. which collects play data by interacting with all the present
% objects in a random order.\textcolor{red}{TODO(Patrick)}

% % \textbf{Real-world setting.} Randomly selected workspaces in our real-world environment is shown in \textcolor{red}{TODO(Patrick): Add figure}. Here, a Sawyer robot is controlled at \textcolor{red}{TODO(Patrick): ?}Hz. Our real-world setting mirrors our simulated setting in that both share the same robotic manipulation tasks, sampling scheme for generating a new environment, and degrees of control of the robot.

% % Our real-world dataset consists of \textcolor{red}{TODO(Patrick): ?} trajectories (x transitions) collected by using a 3Dconnexion SpaceMouse device. Instructions for interfacing with the SpaceMouse is available publicly on  \href{https://github.com/vitchyr/rlkit/tree/master/rlkit/demos/spacemouse}{Github}, with the
% % device code adapted from the RoboSuite library \cite{zhu2020robosuite}. \textcolor{red}{TODO(Patrick): More description here once we figure out the real world stuff



% \textcolor{red}{Patrick (Real World Details)}: The three target tasks are:
% \begin{enumerate}
%     \item Close the drawer and push can in front of drawer
%     \item Push can out of drawer's way and open drawer
%     \item Poke object out to the right of the drawer and close drawer
% \end{enumerate}

% For Task 1, we consider a rollout a success if the robot closes the fully-opened drawer more than halfway and pushes the can in front of the drawer. The pretrained policy has some success with closing the drawer from the initial position (37.5\%). The failure case arises with the transition between closing the drawer and pushing the can. The gripper must both rotate 90 degrees and go behind the can in order to push it in front of the drawer. The pretrained policy often struggles with this as it is easy to land upon unseen states during this transition. For instance, the gripper moves too far back in the scene during the transition and cannot recover by moving forward towards the can. Instead it gets stuck and performs seemingly random actions in that area. After finetuning, success rate of drawer opening increases (75\%). More importantly, the policy has an increased success in transitioning between closing the drawer and pushing the object. 

% Interestingly, the pretrained policy also has many different strategies initially on how to transition due to the diversity in the prior data. Sometimes, it tries to go over and behind the can. Other times, it tries to go around and behind the can. After finetuning, it converges upon the strategy of going around and behind the can, likely since there was more success doing this in the earlier epochs of finetuning. Also, the gripper learns a novel strategy to hit singularity in order to push the drawer by over rotating past its physical limits. \textcolor{red}{(Maybe best not to include this actually)} Note that in the prior data, the robot never hits singularity.

% For Task 2, we consider a rollout a success if the robot pushes the can out of the drawer's way and opens the drawer at least halfway. The pretrained policy has great success with sliding the can (87.5\% success), but it again struggles to transition to opening the drawer after doing the can sliding. When the the gripper starts moving back to the handle after sliding the can, it often goes too far in and collides with the side of the drawer. If this unseen state is reached, the robot does not recover and gets stuck pushing the side of the drawer. Rarely, the robot is able to lucky not go too far back during the transition and reach the handle. If the robot can make it to this state of being close to the handle, it can very consistently open the drawer. After finetuning, the robot has greater success in not going too far back during the transition and getting stuck. Note that to get this working, we had to constrain the rotation of the gripper within $\pi / 2$ to prevent it from hitting singularity. Also we had to constrain the z-dimension so that the robot cannot move past the top of the drawer. If we don't set these constraints, the robot often ends up drifting up too high (where these states are unseen) and cannot recover, instead just wandering around aimlessly.

% For Task 3, we consider a rollout a success if the robot can poke the object to the side of the drawer and close the drawer fully. Note that if the robot pokes the object to different location like the front of the drawer, we consider this to be a failure. The majority of the time, the pretrained policy either fails to close the drawer (37.5\%) or can close the drawer but ignores the object in doing so (50\%). As a result, the object either gets stuck between the gripper and drawer or the object accidently falls onto the table, but often not in the desired location. For instance, it often rolls to the front of the scene. After finetuning, the policy can consistently move rightward explicitly first to push the object on top the table to the right of the drawer, and then close the drawer. Note that we had to constrain the rotation of the gripper within $\pi / 2$ to prevent it from hitting singularity for this task as well.

% When conditioned on only the final goal image after finetuning, the robot goes through the correct high-level motions of the two skills and the transition between them. However, it does this very unprecisely and never succeeds on each skill or the transition between the two skills.

% With GCP, on Task 1 and 2, GCP is never able to succeed. It is able to slide the can, but it fails at opening/closing the drawer. Finetuning doesn't seem to consistently improve the performance of sliding the can, likely since performance is bottlenecked by the noisiness of the subgoals. For Task 3 with GCP, the policy is able to close the drawer the vast majority of the time. However, it runs into the same issue after pretraining where it ignores the object. After finetuning, the policy does a little better at this, although it still ignores the object most of the time (62.5\%). GCP likely works for this task since it is a shorter-horizon task with an easier transition between the two primitive skills. As a result, the model is able to lean more on the goal-conditioned policy and less on the planned subgoals to solve the task.