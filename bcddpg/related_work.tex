Learning methods for decision making problems such as robotics largely divide into two classes:  imitation learning and reinforcement learning (RL).  In imitation learning (also called learning from demonstrations) the agent receives behavior examples from an expert and attempts to solve a task by copying the expert's behavior. In RL, an agent attempts to maximize expected reward through interaction with the environment. Our work combines aspects of both to solve complex tasks.

\textbf{Imitation Learning:} Perhaps the most common form of imitation learning is behavior cloning (BC), which learns a policy through supervised learning on demonstration state-action pairs.  BC has seen success in autonomous driving~\citep{pomerleau1989alvinn, bojarski2016nvidia}, quadcopter navigation~\citep{giusti15trails}, locomotion~\citep{nakanishi2004bipedlfd, kalakrishnan09terraintemplates}.
BC struggles outside the manifold of demonstration data. Dataset Aggregation (DA\scriptsize{GGER}\normalsize) augments the dataset by interleaving the learned and expert policy to address this problem of accumulating errors~\citep{ross2011dagger}. However, DA\scriptsize{GGER}\normalsize \, is difficult to use in practice as it requires access to an expert during all of training, instead of just a set of demonstrations.

Fundamentally, BC approaches are limited because they do not take into account the task or environment. Inverse reinforcement learning (IRL)~\citep{ng2000irl} is another form of imitation learning where a reward function is inferred from the demonstrations. Among other tasks, IRL has been applied to navigation~\citep{ziebart2008maxent}, autonomous helicopter flight~\citep{abbeel2004apprenticeship}, and manipulation~\citep{finn16guidedcostlearning}. Since our work assumes knowledge of a reward function, we omit comparisons to IRL approaches.

\textbf{Reinforcement Learning:} Reinforcement learning methods have been harder to apply in robotics, but are heavily investigated because of the autonomy they could enable. Through RL, robots have learned to play table tennis~\citep{peters2010reps}, swing up a cartpole, and balance a unicycle~\citep{deisenroth2011pilco}. A renewal of interest in RL cascaded from success in games~\citep{mnih2015human, Silver2016}, especially because of the ability of RL with large function approximators (ie. deep RL) to learn control from raw pixels. Robotics has been more challenging in general but there has been significant progress. Deep RL has been applied to manipulation tasks~\citep{LevineFDA15}, grasping~\citep{pinto2015supersizing, levine2016learning}, opening a door~\citep{Gu2016b}, and locomotion~\citep{lillicrap2015continuous, mnih2016asynchronous, schulman2015trpo}.  However, results have been attained predominantly in simulation per high sample complexity, typically caused by exploration challenges.

\textbf{Robotic Block Stacking:} Block stacking has been studied from the early days of AI and robotics as a task that encapsulates many difficulties of more complicated tasks we want to solve, including multi-step planning and complex contacts. SHRDLU~\citep{winograd72shrdlr} was one of the pioneering works, but studied block arrangements only in terms of logic and natural language understanding. More recent work on task and motion planning considers both logical and physical aspects of the task~\citep{Kaelbling2011, Kavraki1996, srivastava14tamp}, but requires domain-specific engineering. In this work we study how an agent can learn this task without the need of domain-specific engineering.

One RL method, PILCO~\citep{deisenroth2011pilco} has been applied to a simple version of stacking blocks where the task is to place a block on a tower~\citep{deisenroth2011blocks}. Methods such as PILCO based on learning forward models naturally have trouble modelling the sharply discontinuous dynamics of contacts; although they can learn to place a block, it is a much harder problem to grasp the block in the first place.  One-shot Imitation~\citep{duan2017oneshotimitation} learns to stack blocks in a way that generalizes to new target configurations, but uses more than 100,000 demonstrations to train the system. A heavily shaped reward can be used to learn to stack a Lego block on another with RL~\citep{popov17stacking}. In contrast, our method can succeed from fully sparse rewards and handle stacking several blocks.

\textbf{Combining RL and Imitation Learning:} 
Previous work has combined reinforcement learning with demonstrations. Demonstrations have been used to accelerate learning on classical tasks such as cart-pole swing-up and balance~\citep{schaal97lfd}. This work initialized policies and (in model-based methods) initialized forward models with demonstrations. Initializing policies from demonstrations for RL has been used for learning to hit a baseball~\citep{peters2008baseball} and for underactuated swing-up~\citep{kober2008mp}. Beyond initialization, we show how to extract more knowledge from demonstrations by using them effectively throughout the entire training process.

Our method is closest to two recent approaches --- Deep Q-Learning From Demonstrations (DQfD)~\citep{hester17dqfd} and DDPG From Demonstrations (DDPGfD)~\citep{vecerik17ddpgfd} which combine demonstrations with reinforcement learning. DQfD improves learning speed on Atari, including a margin loss which encourages the expert actions to have higher Q-values than all other actions. This loss can make improving upon the demonstrator policy impossible which is not the case for our method. Prior work has previously explored improving beyond the demonstrator policy in simple environments by introducing slack variables~\citep{kim2013apid}, but our method uses a learned value to actively inform the improvement. DDPGfD solves simple robotics tasks akin to peg insertion using DDPG with demonstrations in the replay buffer. In contrast to this prior work, the tasks we consider exhibit additional difficulties that are of key interest in robotics: multi-step behaviours, and generalization to varying goal states.
While previous work focuses on speeding up already solvable tasks, we show that we can extend the state of the art in RL with demonstrations by introducing new methods to incorporate demonstrations.