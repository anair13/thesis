\section{Related Work}\label{sec:relatedwork}

Learning has been applied previously in a variety of robotics contexts. Different forms of learning have enabled autonomous driving \cite{pomerleau1989alvinn}, biped locomotion \cite{nakanishi2004bipedlfd}, block stacking \cite{deisenroth2011stacking}, grasping \cite{pinto2015supersizing}, and navigation \cite{giusti15trails, pathak2018zeroshot}. Among these methods, many involve reinforcement learning, where an agent learns to perform a task by maximizing a reward signal. Reinforcement learning algorithms have been developed and applied to teach robots to perform tasks such as balancing a robot \cite{deisenroth2011pilco}, playing ping-pong \cite{peters2010reps} and baseball \cite{peters2008baseball}.
The use of large function approximators, such as neural networks, in RL has further broadened the generality of RL \cite{mnih2013atari}. Such techniques, called ``deep'' RL, have further allowed robots to be trained directly in the real world to perform fine-grained manipulation tasks from vision \cite{levine2016gps}, open doors \cite{gu2016naf}, play hockey \cite{chebotar2017pilqr}, stack Lego blocks \cite{zhang2019solar}, use dexterous hands \cite{zhu2019hands}, and grasp objects \cite{kalashnikov2018qtopt}. In this work we further explore solving real-world robotics tasks using RL.

Many RL algorithms introduce prior information about the specific task to be solved. One common method is reward shaping \cite{ng1999rewardshaping}, but reward shaping can become arbitrarily difficult as the complexity of the task increases. Other methods incorporate a trajectory planner \cite{thomas2018cad} but for complex assembly tasks, trajectory planners require a host of information about objects and geometries which can be difficult to provide.

Another body of work on incorporating prior information studies using  demonstrations either to initialize a policy \cite{peters2008baseball, kober2008mp}, infer reward functions using inverse reinforcement learning \cite{finn16guidedcostlearning, ziebart2008maxent} or to improve the policy throughout the learning procedure \cite{hester17dqfd, nair2018demonstrations, rajeswaran2018dextrous}. These methods require multiple demonstrations, which can be difficult to collect, especially for assembly tasks, although learning a reward function by classifying goal states \cite{singh2019raq} may partially alleviate this issue. More recently, manually specifying a policy and learning the residual task has been proposed \cite{johannink18residualrl, silver18residualpolicylearning}. In this work we evaluate both residual RL and combining RL with learning from demonstrations.

Previous work has also tackled high precision assembly tasks, especially insertion-type tasks. One line of work focuses on obtaining high dimensional observations, including geometry, forces, joint positions and velocities \cite{li2014usbgelsight, tamar2017hindsightplan, inoue2017deeprlassembly, luo19variableimpedance}, but this information is not easily procured, increasing complexity of the experiments and the supervision required. Other work relies on external trajectory planning or very high precision control \cite{inoue2017deeprlassembly, tamar2017hindsightplan}, but this can be brittle to error in other components of the system, such as perception. We show how our method not only solves insertion tasks with much less information about the environment, but also does so under noisy conditions.